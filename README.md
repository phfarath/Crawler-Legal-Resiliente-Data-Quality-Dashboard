# Crawler Legal Resiliente + Data Quality Dashboard

![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Docker](https://img.shields.io/badge/docker-ready-blue)

Sistema de crawling robusto e resiliente para coleta automatizada de decisÃµes judiciais dos principais portais brasileiros (PJe, eSAJ, eProc) com dashboard de qualidade de dados e monitoramento em tempo real.

## ğŸš€ Quick Start

### PrÃ©-requisitos
- Python 3.9+
- Docker e Docker Compose
- Git

### InstalaÃ§Ã£o RÃ¡pida

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/crawler-legal-resiliente.git
cd crawler-legal-resiliente

# Configure variÃ¡veis de ambiente
cp .env.example .env

# Inicie os serviÃ§os com Docker
docker-compose up -d

# Acesse o dashboard
open http://localhost:8501
```

## âš¡ Funcionalidades Principais

### 1. Crawling Resiliente
- âœ… Retry automÃ¡tico com backoff exponencial
- âœ… RotaÃ§Ã£o de proxies e user-agents
- âœ… DetecÃ§Ã£o e resoluÃ§Ã£o de CAPTCHAs
- âœ… Fallback entre Scrapy/Selenium
- âœ… Auto-recuperaÃ§Ã£o de falhas estruturais

### 2. Qualidade de Dados
- âœ… ValidaÃ§Ã£o de schemas com Pydantic
- âœ… NormalizaÃ§Ã£o automÃ¡tica (CNJ, datas, textos)
- âœ… DeduplicaÃ§Ã£o (hash exato + MinHash fuzzy)
- âœ… Enriquecimento com metadados derivados

### 3. Monitoramento
- âœ… Dashboard Streamlit em tempo real
- âœ… MÃ©tricas Prometheus
- âœ… Alertas automÃ¡ticos (Slack, email, Sentry)
- âœ… DetecÃ§Ã£o de mudanÃ§as estruturais nos portais

### 4. Arquitetura Cloud-Ready
- âœ… Deploy via Docker/Docker Compose
- âœ… Suporte para GCP e AWS
- âœ… Message queue com RabbitMQ + Celery
- âœ… Multi-database (PostgreSQL, MongoDB, Redis, OpenSearch)

## ğŸ“Š Stack TecnolÃ³gica

| Categoria | Tecnologias |
|-----------|-------------|
| **Crawling** | Scrapy, Selenium, undetected-chromedriver |
| **Anti-Bot** | fake-useragent, playwright-stealth, 2captcha |
| **Qualidade** | Pydantic, pandas, fuzzywuzzy, datasketch |
| **Databases** | PostgreSQL, MongoDB, Redis, OpenSearch |
| **Queue** | RabbitMQ, Celery, Flower |
| **Dashboard** | Streamlit, Plotly |
| **API** | FastAPI |
| **Deploy** | Docker, Docker Compose |

## ğŸ“ Estrutura do Projeto

```
crawler-legal-resiliente/
â”œâ”€â”€ src/                        # CÃ³digo-fonte principal
â”‚   â”œâ”€â”€ crawlers/              # Spiders (PJe, eSAJ, eProc)
â”‚   â”œâ”€â”€ pipelines/             # Processamento de dados
â”‚   â”œâ”€â”€ models/                # Modelos e schemas
â”‚   â”œâ”€â”€ database/              # Camadas de persistÃªncia
â”‚   â”œâ”€â”€ services/              # ServiÃ§os de negÃ³cio
â”‚   â”œâ”€â”€ utils/                 # UtilitÃ¡rios
â”‚   â”œâ”€â”€ config/                # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ tasks/                 # Tasks Celery
â”œâ”€â”€ dashboard/                  # Dashboard Streamlit
â”œâ”€â”€ api/                       # API REST
â”œâ”€â”€ tests/                     # Testes automatizados
â”œâ”€â”€ docker-compose.yml         # OrquestraÃ§Ã£o de containers
â””â”€â”€ requirements.txt           # DependÃªncias Python
```

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Copie `.env.example` para `.env` e ajuste conforme necessÃ¡rio:

```bash
# Principais configuraÃ§Ãµes
ENVIRONMENT=development
PROXY_ENABLED=false
CAPTCHA_API_KEY=chave-api-servico
SLACK_WEBHOOK_URL=chave-api-servico
SENTRY_DSN=chave-api-servico
```

### Bancos de Dados

O projeto usa mÃºltiplos bancos para diferentes propÃ³sitos:

- **PostgreSQL**: Metadados estruturados (decisÃµes, partes, datas)
- **MongoDB**: Documentos brutos (HTML, JSON)
- **Redis**: Cache, sessÃµes, deduplicaÃ§Ã£o
- **OpenSearch**: Busca full-text

Todos sÃ£o provisionados automaticamente via Docker Compose.

## ğŸƒ ExecuÃ§Ã£o

### Local (Desenvolvimento)

```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Iniciar bancos via Docker
docker-compose up -d postgres mongodb redis rabbitmq opensearch

# Executar spider
scrapy crawl tjsp_esaj

# Iniciar dashboard
streamlit run dashboard/app.py

# Iniciar API
uvicorn api.main:app --reload
```

### Docker (ProduÃ§Ã£o)

```bash
# Build e start completo
docker-compose up -d

# Escalar workers
docker-compose up -d --scale crawler-worker=5

# Ver logs
docker-compose logs -f dashboard

# Parar tudo
docker-compose down
```

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest

# Com coverage
pytest --cov=src --cov-report=html

# Apenas testes unitÃ¡rios
pytest tests/unit/

# Testes especÃ­ficos
pytest tests/unit/test_cnj_utils.py -v
```

## ğŸ“Š Dashboard

Acesse `http://localhost:8501` para visualizar:

- **Overview**: Total de processos, taxa de sucesso, cobertura
- **Qualidade**: Duplicatas, completude, validaÃ§Ãµes
- **Portais**: Status e latÃªncia de cada portal
- **Alertas**: NotificaÃ§Ãµes de erros e falhas

## ğŸ” API REST

Acesse `http://localhost:8000/docs` para documentaÃ§Ã£o interativa.

Principais endpoints:

```
GET /health              # Health check
GET /api/v1/decisions    # Listar decisÃµes
GET /api/v1/decisions/{cnj}  # Buscar por nÃºmero CNJ
GET /api/v1/metrics/quality  # MÃ©tricas de qualidade
```

## ğŸ“ Desenvolvimento

### Code Style

O projeto segue PEP8 com Black + isort + flake8:

```bash
# Formatar cÃ³digo
black src/ tests/

# Ordenar imports
isort src/ tests/

# Linter
flake8 src/ tests/

# Type checking
mypy src/
```

### Pre-commit Hooks

```bash
# Instalar hooks
pre-commit install

# Executar manualmente
pre-commit run --all-files
```

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- Projeto desenvolvido para demonstrar boas prÃ¡ticas em web scraping, qualidade de dados e observabilidade
- Stack tecnolÃ³gico baseado em ferramentas production-grade usadas pela indÃºstria

## ğŸ“ Contato

- **DocumentaÃ§Ã£o Completa**: Ver [README-detailed.md](README-detailed.md)
- **Issues**: https://github.com/seu-usuario/crawler-legal-resiliente/issues

---

**âš–ï¸ Desenvolvido com foco em resiliÃªncia, qualidade e observabilidade**
